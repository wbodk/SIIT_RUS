{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "def func(x):\n",
    "    return 1/2*(x[0]**2 + x[1]**2)\n",
    "\n",
    "def dfunc1(x):\n",
    "    return x[0]\n",
    "\n",
    "def dfunc2(x):\n",
    "    return x[1]\n",
    "\n",
    "def gradf(x):\n",
    "    x = [dfunc1(x), dfunc2(x)]\n",
    "    x = np.array(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Метод наискорейшего спуска**\n",
    "\n",
    "Этот метод используется для поиска минимума дифференцируемой функции \n",
    "$ f(x) = f(x_1, x_2, \\ldots, x_n) $, смещая текущее решение в направлении отрицательного градиента \n",
    "$ \\nabla f = [\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}]^T $ на каждой итерации.\n",
    "\n",
    "Метод состоит из следующих шагов:\n",
    "\n",
    "**Инициализация:** Выбираются начальное приближение $ x_0 $, размер шага $ \\gamma > 0 $, допустимая погрешность $ \\varepsilon > 0 $, и максимальное количество итераций $ N $.\n",
    "\n",
    "**Тело метода (итеративное применение):** На $ k $-ой итерации у нас есть\n",
    "$ x_{k+1} = x_k - \\gamma \\nabla f(x_k) $\n",
    "\n",
    "**Критерий остановки:** По окончании каждой итерации проверяем условие $ \\|\\nabla f(x_k)\\| \\leq \\varepsilon $. Когда это условие выполняется или когда мы достигаем максимального допустимого числа итераций, мы прекращаем выполнение алгоритма.\n",
    "\n",
    "$\\gamma$ (шаг обучения): Значение gamma обычно выбирается в диапазоне от 0.1 до 0.9. Если ваш шаг обучения слишком большой, алгоритм может не сойтись, и, наоборот, если слишком мал, сходимость может быть медленной. Попробуйте начать с относительно небольшого значения, например, 0.1, и увеличивайте его, пока алгоритм не начнет сходиться.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.96841967e-05]\n",
      " [7.96841967e-05]]\n"
     ]
    }
   ],
   "source": [
    "def steepest_descent(gradf, x0, gamma, epsilon, N):\n",
    "    x = np.array(x0).reshape(len(x0), 1)\n",
    "    for k in range(N):\n",
    "        g = gradf(x)\n",
    "        x = x - gamma*g\n",
    "        if np.linalg.norm(g) < epsilon:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "gamma = 0.1\n",
    "epsilon = 1e-4\n",
    "max_steps = 100\n",
    "x0 = [3,3]\n",
    "\n",
    "print(steepest_descent(gradf, x0, gamma, epsilon, max_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Градиентный метод с моментом**\n",
    "\n",
    "В методе градиентного спуска с моментом, общая структура алгоритма остается неизменной, но текущая позиция в процессе поиска обновляется немного измененным способом:\n",
    "\n",
    "## $\\mathbf{v}_k = \\omega \\mathbf{v}_{k-1} + \\gamma \\nabla f(\\mathbf{x}_k)$\n",
    ">>\n",
    "## $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\mathbf{v}_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$ (шаг обучения): Значение gamma обычно выбирается в диапазоне от 0.1 до 0.9. Если ваш шаг обучения слишком большой, алгоритм может не сойтись, и, наоборот, если слишком мал, сходимость может быть медленной. Попробуйте начать с относительно небольшого значения, например, 0.1, и увеличивайте его, пока алгоритм не начнет сходиться.\n",
    "\n",
    "$\\omega$ (коэффициент момента): Значение omega также обычно находится в диапазоне от 0.1 до 0.9. Можно начать с маленького значения, например, 0.1, и постепенно увеличивать, чтобы увидеть, как это влияет на производительность. Момент обычно помогает устранить осцилляции и ускорить сходимость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78658039e-09, -5.95526798e-11]), 24)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def steepest_descent_with_momentum(gradf, x0, gamma, epsilon, omega, max_iterations):\n",
    "    x0 = np.array(x0)\n",
    "    v = 0\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        g = gradf(x0)\n",
    "        v = omega*v + gamma*g\n",
    "        x0 = x0 - v\n",
    "        if np.linalg.norm(g) < epsilon:\n",
    "            break\n",
    "    return x0, i+1\n",
    "\n",
    "x0 = [-3,-0.1]\n",
    "gamma = 0.7*0.8 ## разбиваем множитель для gamma и omega (который в сумме дает 1) на доли для gamma и omega\n",
    "epsilon = 1e-8\n",
    "omega = 0.1*0.2 \n",
    "max_iterations = 1000\n",
    "\n",
    "steepest_descent_with_momentum(gradf, x0, gamma, epsilon, omega, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ADAGRAD**\n",
    "\n",
    "Adagrad использует адаптивный градиент, специфичный для каждой оси (каждой переменной).\n",
    "\n",
    "Пусть $ g_{k, i} $ - градиент критерия оптимальности по $ i $-й переменной в $ k $-й итерации,\n",
    "\n",
    "### $ G_{k, i} = \\sum_{j=1}^{k} (g_{j, i})^2 $\n",
    "\n",
    "где $ G_{k, i} $ - сумма квадратов градиентов по $ i $-й переменной до $ k $-й итерации.\n",
    "\n",
    "Обновление $ i $-й переменной:\n",
    "\n",
    "### $ x_{k+1, i} = x_{k, i} - \\frac{\\gamma}{\\sqrt{G_{k, i} + \\epsilon}} g_{k, i} $\n",
    "\n",
    "где $ \\gamma $ - скорость обучения, $ \\epsilon $ - малая константа, предотвращающая деление на ноль.\n",
    "\n",
    "Этот метод позволяет каждой переменной адаптивно регулировать скорость обучения, учитывая её историю градиентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$ (learning rate): Значение gamma (learning rate) обычно выбирается в диапазоне от 0.01 до 0.1. Если ваш шаг обучения слишком большой, это может привести к расходимости, а если слишком мал, оптимизация может быть слишком медленной. Начните с относительно небольшого значения и, при необходимости, постепенно увеличивайте.\n",
    "\n",
    "$\\epsilon$ (эпсилон для стабилизации): Значение eta обычно выбирается в диапазоне от 1e-8 до 1e-4. Эта константа добавляется в знаменатель в формуле обновления, чтобы избежать деления на очень маленькие значения. Начните с небольшого значения, например, 1e-8, и увеличивайте, если это необходимо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.10570401e-09, 2.92681603e-73]), 72)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adagrad(gradf, x0, gamma, eta, epsilon, max_iterations):\n",
    "    x0 = np.array(x0)\n",
    "    v = np.zeros(len(x0))\n",
    "    G = np.zeros(len(x0))\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        g = gradf(x0)\n",
    "        G += np.multiply(g, g)\n",
    "        v = gamma*np.ones_like(G) / np.sqrt(G+eta) * g\n",
    "        x0 = x0 - v\n",
    "        if (np.linalg.norm(g) < epsilon):\n",
    "            break\n",
    "        \n",
    "    return x0, i\n",
    "\n",
    "x0 = [3,-0.1]\n",
    "gamma = 1\n",
    "epsilon = 1e-8\n",
    "eta = 1e-8 ## Порядка epsilon\n",
    "max_iterations = 1000\n",
    "\n",
    "adagrad(gradf, x0, gamma, eta, epsilon, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ADAM**\n",
    "\n",
    "ADAM (ADAPTIVE MOMENT ESTIMATION) - одна из наиболее широко используемых современных модификаций алгоритма наискорейшего спуска.\n",
    "\n",
    "Сначала определяются вспомогательные величины:\n",
    "\n",
    "### $ m_k = \\omega_1 m_{k-1} + (1 - \\omega_1) g_k $\n",
    "### $ v_k = \\omega_2 v_{k-1} + (1 - \\omega_2) g_{k}^2 $\n",
    "\n",
    "и их скорректированные версии:\n",
    "\n",
    "### $ \\hat{m}_k = \\frac{m_k}{1 - \\omega_1^k} $\n",
    "### $ \\hat{v}_k = \\frac{v_k}{1 - \\omega_2^k} $\n",
    "\n",
    "Затем текущее решение обновляется по алгоритму:\n",
    "\n",
    "### $$ x_{k+1} = x_k - \\frac{\\gamma}{\\sqrt{\\hat{v}_k + \\epsilon}} \\hat{m}_k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$ (learning rate): Значение gamma (learning rate) обычно выбирается в диапазоне от 1e-5 до 0.1. Можно начать с небольшого значения, например, 0.001, и затем экспериментировать с изменением этого значения в зависимости от производительности. Если процесс сходится слишком медленно или не сходится вовсе, увеличьте learning rate.\n",
    "\n",
    "$\\omega_1$ (экспоненциальное затухание для момента первого порядка): Значение omega1 обычно лежит в диапазоне от 0.8 до 0.999. Чем ближе к 1, тем больше веса отдаются предыдущим моментам первого порядка. Можно начать с 0.9 и далее настраивать в соответствии с результатами.\n",
    "\n",
    "$\\omega_2$ (экспоненциальное затухание для момента второго порядка): Значение omega2 также обычно лежит в диапазоне от 0.8 до 0.999. Можно начать с 0.999, и по мере необходимости уменьшать это значение. Значение близкое к 1 дает больший вес более новым значениям второго момента.\n",
    "\n",
    "$\\epsilon$ (эпсилон для стабилизации): Значение eta обычно выбирается в диапазоне от 1e-8 до 1e-4. Можно начать с 1e-8 и увеличивать, если необходимо. Эта константа добавляется к знаменателю в формуле коррекции шага для избежания деления на очень маленькие значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.06181367e-04,  2.05860890e-05]), 72)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adam(gradf, x0, gamma, omega1, omega2, eta, max_iterations, epsilon):\n",
    "    x0 = np.array(x0)\n",
    "    m = np.ones_like(x0)\n",
    "    v = np.ones_like(x0)\n",
    "    for i in range(max_iterations):\n",
    "        g = gradf(x0)\n",
    "\n",
    "        v = v * omega2 + (1 - omega2) * np.multiply(g, g)\n",
    "        adj_v = v / (1 - omega2)\n",
    "        gamma_adj = gamma * np.ones_like(x0) / (np.sqrt(adj_v + eta))\n",
    "\n",
    "        m = m * omega1 + (1 - omega1) * g\n",
    "        adj_m = m / (1 - omega1)\n",
    "\n",
    "        x0 = x0 - np.multiply(gamma_adj, adj_m)\n",
    "\n",
    "        if np.linalg.norm(g) < epsilon:\n",
    "            break\n",
    "    return x0, i\n",
    "\n",
    "x0 = [3,0.1]\n",
    "gamma = 0.45 ## скорость обучения\n",
    "omega1, omega2 = 0.79, 0.99 ## omega1 наиболее близкая к 1, разница между omega1 omega2 влияет на скорость обучения\n",
    "epsilon = 1e-4\n",
    "eta = 1e-4 ## порядка epsilon\n",
    "max_iterations = 10000\n",
    "\n",
    "adam(gradf, x0, gamma, omega1, omega2, eta ,max_iterations, epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
