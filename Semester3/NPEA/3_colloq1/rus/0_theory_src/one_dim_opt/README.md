# 1. Метод Ньютона-Рапсона (newton_rhapson.py)
**Метод Ньютона-Рапсона** — это численный метод решения уравнений, который использует итерационный процесс для приближенного нахождения корня уравнения. Алгоритм метода следующий:

1. **Начальный выбор:**
   - Выбирается начальное приближение \(x_0\) к корню уравнения.

2. **Итерационный процесс:**
   - На каждом шаге \(n\) вычисляется следующее приближение \(x_{n+1}\) по формуле:

\[
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
\]

   где:
   - \(f(x_n)\) — значение функции в точке \(x_n\),
   - \(f'(x_n)\) — значение производной функции в точке \(x_n\).

3. **Обновление:**
   - Процесс повторяется, и каждый раз вычисляется новое приближение корня.

4. **Проверка сходимости:**
   - Итерации продолжаются до тех пор, пока не будет достигнута заданная точность (критерий сходимости) или выполнено другое условие останова.

Метод Ньютона-Рапсона имеет квадратичную скорость сходимости, что означает, что с каждой итерацией удваивается количество верных знаков в приближении к корню. Однако, он требует наличия производной функции, что может быть вызовом для сложных функций или в случае, когда вычисление производной затруднительно.
# 2. Метод секущих (secant.p)
**Метод секущих (метод хорд)** — это численный метод решения уравнений, аппроксимирующий корень путем построения хорды (отрезка) на графике функции и использования ее для приближенного определения корня. Алгоритм метода следующий:

1. **Начальный выбор:**
   - Выбираются две начальные точки \(x_0\) и \(x_1\), близкие к корню уравнения.

2. **Итерационный процесс:**
   - Строится хорда между точками \((x_0, f(x_0))\) и \((x_1, f(x_1))\).
   - Новая аппроксимация корня \(x_2\) находится как пересечение хорды с осью \(x\).

\[
x_2 = x_1 - \frac{f(x_1) \cdot (x_1 - x_0)}{f(x_1) - f(x_0)}
\]

3. **Обновление:**
   - Точки \(x_0\) и \(x_1\) обновляются: \(x_0 = x_1\), \(x_1 = x_2\).

4. **Повторение:**
   - Процесс повторяется до сходимости (достаточного приближения к корню) или до выполнения другого критерия останова.

Метод секущих аппроксимирует производную функции при помощи разностного приближения, и, в отличие от метода Ньютона, не требует вычисления производной в явной форме. Однако, он может быть менее быстрым в сходимости по сравнению с методом Ньютона.
# 3. Метод Фибоначчи (fib_opt_search.py)
**Метод Фибоначчи** — это численный метод оптимизации, который используется для нахождения минимума (или максимума) функции в заданном интервале. Алгоритм метода включает следующие шаги:

1. **Выбор начальных точек:**
   - Задаются начальные границы интервала \([a, b]\) и точность \(\varepsilon\).

2. **Инициализация ряда Фибоначчи:**
   - Находится такое число \(n\), что \(F_n \geq \frac{b - a}{\varepsilon}\), где \(F_n\) — \(n\)-е число Фибоначчи.
   - Выбираются начальные точки \(x_1\) и \(x_2\) внутри интервала так, чтобы длина интервала была равна \(\frac{b - a}{F_n}\).

3. **Итерационный процесс:**
   - На каждом шаге процесса точки \(x_1\) и \(x_2\) обновляются в зависимости от того, какая из подинтервалов \([a, x_2]\) или \([x_1, b]\) содержит минимум (максимум) функции.
   - Длина выбранного подинтервала уменьшается в соответствии с последовательностью чисел Фибоначчи: \(F_n\) умножается на \(\frac{F_{n-2}}{F_{n-1}}\).
   - Процесс повторяется до достижения заданной точности.

4. **Завершение:**
   - Процесс завершается, когда длина текущего интервала становится меньше или равной \(\varepsilon\).

Метод Фибоначчи основан на идее последовательного уточнения интервала, содержащего оптимальное решение, и может быть использован для оптимизации унимодальных функций. Однако, в сравнении с некоторыми другими методами оптимизации, метод Фибоначчи может требовать больше вычислительных ресурсов.

# 4. Метод Золотого сечения (gold_ratio.py)
**Метод Золотого сечения** — это численный метод оптимизации, используемый для нахождения локального минимума (или максимума) унимодальной функции в заданном интервале. Алгоритм метода включает следующие шаги:

1. **Выбор начальных точек:**
   - Задаются начальные границы интервала \([a, b]\) и точность \(\varepsilon\).

2. **Инициализация точек:**
   - Выбираются две промежуточные точки \(x_1\) и \(x_2\) внутри интервала так, чтобы отношение длин отрезков \([a, x_2]\) и \([x_1, b]\) было равно золотому сечению, т.е.,

\[
\frac{b - x_2}{x_2 - a} = \frac{x_2 - a}{b - x_1} = \phi
\]

где \(\phi\) — золотое сечение, приближенно равное 1.618.

3. **Итерационный процесс:**
   - Оценивается значение функции в точках \(f(x_1)\) и \(f(x_2)\).
   - Сравниваются значения функции в точках \(x_1\) и \(x_2\), и один из концов интервала сужается в сторону, где функция принимает меньшее значение.
   - Процесс повторяется, пока длина текущего интервала становится меньше или равной \(\varepsilon\).

4. **Завершение:**
   - Процесс завершается, когда достигается заданная точность.

Метод Золотого сечения обладает свойством сходиться к оптимальному решению экспоненциально быстро. Этот метод особенно эффективен для унимодальных функций, т.е., функций, имеющих только один локальный экстремум.
