{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearna regresija - uvod\n",
    "\n",
    "## Jednostavna linearna regresija\n",
    "\n",
    "Je statistički metod koji omogućava proučavanje veze između dve kontinualne promenljive.\n",
    "\n",
    "Prva promenljiva, $x$, se naziva **nezavisna promeljiva**. Druga promenljiva, $y$ se naziva **zavisna promenljiva**.\n",
    "\n",
    "Jednostavna linearna regresija ima pridev \"jednostavna\" zato što ima samo jednu nezavisnu promenljivu $x$.\n",
    "Postoji i višestruka linearna regresija, koja se odnosni na linearnu regresiju sa više nezavisnih promenljivih $x_1, x_2,...x_n$.\n",
    "\n",
    "---\n",
    "Pre opisa same linearne regresije, važno je napomenuti kakve veze/zavisnosti između promenljivih su od interesa.\n",
    "\n",
    "**Deterministička (funkcionalna) zavisnost** nam nije od interesa jer njom možemo odrediti **tačnu** vrednost zavisne promenljive $y$ na osnovu vrednosti nezavise promenljive $x$. Primer: konvertovanje mernih jedinica temperature između stepeni celzijusa i farenhajta:\n",
    "\n",
    "$ \\text{Fahr} = \\frac{9}{5} * \\text{Cels} + 32 $\n",
    "\n",
    "![](img/celcius_fahr_plot.gif)\n",
    "\n",
    "Znajući temperaturu u stepenima Celzijusa, možemo iskoristiti prethodnu jednačinu da **tačno** odredimo temperaturu u Farenhajtima. Još primera determinističke zavisnosti:\n",
    "\n",
    "* $ \\text{obim} = 2 \\pi * \\text{poluprecnik} $\n",
    "* Omov zakon: $ \\text{I} = \\frac{V}{R} $\n",
    "* Hukov zakon: $ \\text{F} = -\\text{k}\\text{x} $\n",
    "\n",
    "\n",
    "Za svaku od ovih determinističkih zavisnosti, jednačina tačno opisuje odnos dve promenljive. Ne razmatramo determinističke veze, već  **statističku zavisnost**, gde veza između dve promenljive nije savršeno tačna.\n",
    "\n",
    "Primer statističke zavisnosti je određivanje stope smrtnosti od raka kože (broj smrti na 10 miliona ljudi) na osnovu geografske širine centra svake od 49 država u SAD ([skincancer.csv](data/skincancer.csv))\n",
    "\n",
    "Može se pretpostaviti da u zemljama koje su severnije, ljudi su manje izloženi sunčevoj svetlosti i štetnim uticajima sunčevih zraka (UV zračenja) i samim tim bi rizik smrti od raka kože trebao biti manji. Dole prikazani grafik podržava takvu hipotezu - vidi se negativna linerna zavisnost između geografske širine i stope smrtnosti od raka kože, ali ova veza nije savršeno linearna. Dakle, ovo je statistička zavisnost, a ne deterministička.\n",
    "\n",
    "![](img/scatterplot_skin_cancer.png)\n",
    "\n",
    "Još neki primeri statističke zavisnosti mogu biti:\n",
    "\n",
    "* Visina i težina ljudi - uglavnom očekujemo da su više osobe teže, ali ne mora nužno biti tako\n",
    "* Kapacitet pluća i količina konzumiranih cigara tokom života - kako se količina konzumiranih cigara povećava, očekuje se smanjenje kapaciteta pluća\n",
    "* Broj ljudi u prostoriji i temperatura/vlažnost prostorije\n",
    "* Količina uloženog truda tokom studiranja (kako ovo izmeriti?) i prolaznost / prosek ocena\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Koja je \"najpogodnija linija\"?\n",
    "\n",
    "Pošto je neophodno pronaći neku linearu zavisnost između dve promenljive, nameće se pitanje koja je to najpogodnija linija koji opisuje tu zavisnost? U principu, zanima nas da pronađemo liniju $ \\hat{y} = a x + b $ koja na najbolji način opisuje date podatke. Podaci su dati kao:\n",
    "\n",
    "* $ x_i $ - vrednost nezavisne promenljive za svako merenje $i$,\n",
    "* $ y_i $ - vrednost zavisne promenljive za svako merenje $i$,\n",
    "* $ \\hat{y}_i $ - *predviđena* vrednost zavisne promenljive za svako merenje $i$.\n",
    "\n",
    "Očigledno je da će se $ y_i $ i $ \\hat{y}_i $ razlikovati u određenoj meri (u nekim merenjima manje u nekim merenjima više), ali je potrebno *u proseku* smanjiti ovu razliku. Jednostavna linearna regresija je postupak koji pronalazi najpogodniju liniju koja opisuje zadate podatke $ x_i $ i $ y_i $, minimizujući grešku predikcije $ e_i = y_i - \\hat{y}_i $.\n",
    "\n",
    "\n",
    "## Metod najmanjih kvadrata\n",
    "\n",
    "Linija koja najbolje \"fituje\" (od eng. *fit* - pristajati) podatke će biti ona linija koja najbolje odgovara podacima, tj. najbolje opisuje vezu između nezavisne $x$ i zavisne promenljive $y$.\n",
    "Najbolje fitovana linija ima najmanju grešku predikcije za svako merenje $i$.\n",
    "\n",
    "Jedan od metoda za pronalaženje ovakve linije je **metod najmanjih kvadrata** (eng. *OLS - Ordinary Least Squares*). Ovaj metod minimizuje sumu kvadrata grešaka predikcije (*SSE - Sum of Squares Error*). SSE je metrika koja govori koliko dobro data linija opisuje podatke.\n",
    "\n",
    "* Jednačina linije koja najbolje \"fituje\" je: $ \\hat{y} = a x + b $. Gde je *a* je nagib linije (*slope*) i *b* odsečak na y-osi (*intercept*).\n",
    "* Potrebno je odrediti vrednosti *a* i *b* tako da je suma kvadratnih grešaka predikcije što manja.\n",
    "* Dakle, potrebno je pronaći *a* i *b* koji minimizuju: \n",
    "\n",
    "$$ SSE=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2 = \\sum_{i=1}^{n}(y_i - a x_i - b)^2 $$\n",
    "\n",
    "\n",
    "Kako bismo pronašli *a* i *b* koje minimizuju *SSE*, pronaći ćemo parcijalne izvode od *SSE* u odnosu na *a* i *b* i izjednačiti ih sa nulom, i rešiti ih za *a* i *b*:\n",
    "$ \\frac{\\partial SSE}{\\partial a} = 0 $,  $ \\frac{\\partial SSE}{\\partial b} = 0 $\n",
    "\n",
    "Rešenje:\n",
    "\n",
    "$$\n",
    "a = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^2}=\\frac{Cov(x,y)}{Var(x)}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "b = {1 \\over n } \\sum_{i=1}^{n} (y_i - a x_i) = \\bar{y} - a \\bar{x}\n",
    "$$\n",
    "\n",
    "\n",
    "Pronalaskom koeficijenata $a$ i $b$, dobijamo jednačinu prave koja ima najmanju SSE metriku.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Zadaci\n",
    "\n",
    "**TODO 1**: Implementirati jednostavu linearnu regresiju. Ulazi parametri su liste/vektori *x* i *y*, koji predstavljaju podatke, a povratne vrednosti su *slope* i *intercept* koji predstavlju nagib i presečnu tačku linije koja najbolje \"fituje\" podatke. Fajl [linreg_simple.py](src/linreg_simple.py) -> funkcija `fit(x, y)`.\n",
    "\n",
    "**TODO 2**: Implementirati predviđanje vrednosti y na osnovu jednog podatka x koristeci nagib i presek. Fajl [linreg_simple.py](src/linreg_simple.py) -> funkcija `predict(x, slope, intercept)`.\n",
    "\n",
    "**TODO 3**: Izvršiti linearnu regresiju na primeru predviđanja stope smrtnosti od raka kože na osnovu geografske širine američkih država.\n",
    "* Učitati datoteku *data/skincancer.csv* i implementirati primenu linearne regresije u Python fajlu [todo3.py](src/todo3.py).\n",
    "* Iscrati grafik za ovaj slučaj.\n",
    "* Probati linearnu regresiju nad ovim istim skupom podataka, ali da se umesto geografske širine koristi geografska dužina.\n",
    "Napomena: Neophodno je instalirati biblioteke sa komandom: `pip install -r requirements.txt`\n",
    "\n",
    "**TODO 4**: Primer iz **TODO 3** rešiti pomoću [scikit-learn](https://scikit-learn.org) biblioteke u fajlu [todo4.py](src/todo4.py).\n",
    "\n",
    "`Scikit-learn` je *open-source* Python biblioteka, koja predstavlja jednostavan i efikasan alat za mašinsko učenje i analizu podataka. Ova biblioteka sadrži implementiran razne algoritme i primere za njihovu primenu.\n",
    "\n",
    "\n",
    "#### Višestruka linearna regresija\n",
    "Za razliku od jednostavne linearne regresije koja koristi samo jednu prediktorsku promenljivu *x*, višestruka linearna regresija podrazumeva korišćenje dve ili više prediktorskih promenljivih. Na prethodnom primeru, mogli bismo dodati: kako zavisi stopa smrtnosti od raka kože u odnosu na geografsku širinu države **i** cene kreme za sunčanje, prosečne plate, klime?\n",
    "\n",
    "Višestruka linearna regresija je odličan alat za otkrivanje interesantnih zavisnosti između podataka, koje možda nisu nisu očigledne na prvi pogled.\n",
    "\n",
    "U opštem obliku, višestruka linearna regresija podrazumeva jednačinu:\n",
    "$ \\hat{y} = a + b_1x_1 + b_2x_2 + ... + b_px_p $, gde je neophodno pronaći parametre *a* i *b* na isti način kao kod jednostavne linearne regresije - minimizacijom sume kvadratnih grešaka predikcije $ SSE=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2 $.\n",
    "\n",
    "Više i višestrukoj regresiji na narednim terminima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
