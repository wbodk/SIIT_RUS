{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия - введение\n",
    "\n",
    "## Простая линейная регрессия\n",
    "\n",
    "Это статистический метод, который позволяет изучать связь между двумя непрерывными переменными.\n",
    "\n",
    "Первая переменная, $x$, называется **независимой переменной**. Вторая переменная, $y$, называется **зависимой переменной**.\n",
    "\n",
    "Простая линейная регрессия называется \"простой\", потому что у нее есть только одна независимая переменная $x$.\n",
    "Существует также множественная линейная регрессия, которая относится к линейной регрессии с несколькими независимыми переменными $x_1, x_2,...x_n$.\n",
    "\n",
    "---\n",
    "Прежде чем описывать саму линейную регрессию, важно отметить, какие типы взаимосвязей/зависимостей между переменными представляют интерес.\n",
    "\n",
    "**Детерминистическая (функциональная) зависимость** нас не интересует, потому что с ее помощью мы можем определить **точное** значение зависимой переменной $y$ на основе значений независимой переменной $x$. Пример: конвертация температурных единиц между градусами Цельсия и Фаренгейта:\n",
    "\n",
    "$ \\text{Fahr} = \\frac{9}{5} * \\text{Cels} + 32 $\n",
    "\n",
    "![](img/celcius_fahr_plot.gif)\n",
    "\n",
    "Зная температуру в градусах Цельсия, мы можем использовать предыдущее уравнение, чтобы **точно** определить температуру в Фаренгейтах. Еще примеры детерминистической зависимости:\n",
    "\n",
    "* $ \\text{объем} = 2 \\pi * \\text{радиус} $\n",
    "* Закон Ома: $ \\text{I} = \\frac{V}{R} $\n",
    "* Закон Гука: $ \\text{F} = -\\text{k}\\text{x} $\n",
    "\n",
    "Для каждой из этих детерминистических зависимостей уравнение точно описывает отношение двух переменных. Мы не рассматриваем детерминистические связи, а **статистическую зависимость**, где связь между двумя переменными не является абсолютно точной.\n",
    "\n",
    "Пример статистической зависимости - определение уровня смертности от рака кожи (число смертей на 10 миллионов человек) на основе географической широты центра каждого из 49 штатов США ([skincancer.csv](data/skincancer.csv))\n",
    "\n",
    "Можно предположить, что в странах, которые находятся севернее, люди менее подвергаются солнечному свету и вредному воздействию солнечных лучей (ультрафиолетового излучения), и, следовательно, риск смерти от рака кожи должен быть меньше. Ниже показан график, который поддерживает такую гипотезу - видна отрицательная линейная зависимость между географической широтой и уровнем смертности от рака кожи, но эта связь не является абсолютно линейной. Таким образом, это статистическая зависимость, а не детерминистическая.\n",
    "\n",
    "![](img/scatterplot_skin_cancer.png)\n",
    "\n",
    "Еще несколько примеров статистической зависимости:\n",
    "\n",
    "* Рост и вес людей - в основном мы ожидаем, что более высокие люди будут тяжелее, но это не обязательно так\n",
    "* Емкость легких и количество выкуриваемых сигарет за жизнь - с увеличением количества выкуренных сигарет ожидается снижение емкости легких\n",
    "* Количество людей в комнате и температура/влажность в помещении\n",
    "* Количество вложенных усилий во время учебы (как это измерить?) и проходимость/средний балл\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Какая линия является \"наилучшей\"?\n",
    "\n",
    "Поскольку необходимо найти линейную зависимость между двумя переменными, возникает вопрос, какая из них является наилучшей линией, описывающей эту зависимость? В принципе, нас интересует поиск линии $ \\hat{y} = a x + b $, которая наилучшим образом описывает предоставленные данные. Данные представлены следующим образом:\n",
    "\n",
    "* $ x_i $ - значение независимой переменной для каждого измерения $i$,\n",
    "* $ y_i $ - значение зависимой переменной для каждого измерения $i$,\n",
    "* $ \\hat{y}_i $ - *предсказанное* значение зависимой переменной для каждого измерения $i$.\n",
    "\n",
    "Очевидно, что $ y_i $ и $ \\hat{y}_i $ будут различаться в определенной степени (в некоторых измерениях меньше, в некоторых больше), но необходимо *в среднем* уменьшить эту разницу. Простая линейная регрессия - это процесс поиска наилучшей линии, которая описывает предоставленные данные $ x_i $ и $ y_i $, минимизируя ошибку предсказания $ e_i = y_i - \\hat{y}_i $.\n",
    "\n",
    "## Метод наименьших квадратов\n",
    "\n",
    "Линия, которая лучше всего \"подгоняет\" (англ. *fit* - соответствовать) данные, будет той линией, которая наилучшим образом соответствует данным, т.е. наилучшим образом описывает отношение между независимой переменной $x$ и зависимой переменной $y$.\n",
    "Наилучшая подогнанная линия имеет минимальную ошибку предсказания для каждого измерения $i$.\n",
    "\n",
    "Один из методов нахождения такой линии - **метод наименьших квадратов** (англ. *OLS - Ordinary Least Squares*). Этот метод минимизирует сумму квадратов ошибок предсказания (*SSE - Sum of Squares Error*). SSE - это метрика, которая показывает, насколько хорошо данная линия описывает данные.\n",
    "\n",
    "* Уравнение линии, которая наилучшим образом \"подгоняет\", это: $ \\hat{y} = a x + b $. Где *a* - наклон линии (*slope*), и *b* - отсечка на оси y (*intercept*).\n",
    "* Необходимо определить значения *a* и *b* так, чтобы сумма квадратов ошибок предсказания была как можно меньше.\n",
    "* Следовательно, нужно найти *a* и *b*, которые минимизируют:\n",
    "\n",
    "$$ SSE=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2 = \\sum_{i=1}^{n}(y_i - a x_i - b)^2 $$\n",
    "\n",
    "Чтобы найти *a* и *b*, которые минимизируют *SSE*, найдем частные производные *SSE* по *a* и *b* и приравняем их к нулю, а затем решим уравнения относительно *a* и *b*:\n",
    "$ \\frac{\\partial SSE}{\\partial a} = 0 $,  $ \\frac{\\partial SSE}{\\partial b} = 0 $\n",
    "\n",
    "Решение:\n",
    "\n",
    "$$\n",
    "a = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^2}=\\frac{Cov(x,y)}{Var(x)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = {1 \\over n } \\sum_{i=1}^{n} (y_i - a x_i) = \\bar{y} - a \\bar{x}\n",
    "$$\n",
    "\n",
    "Найдя коэффициенты $a$ и $b$, мы получаем уравнение прямой, которая имеет минимальную метрику SSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Задачи\n",
    "\n",
    "**TODO 1**: Реализовать простую линейную регрессию. Входные параметры - это списки/векторы *x* и *y*, представляющие данные, а возвращаемые значения - *slope* и *intercept*, представляющие наклон и точку пересечения линии, которая лучше всего \"подходит\" данным. Файл [linreg_simple.py](src/linreg_simple.py) -> функция `fit(x, y)`.\n",
    "\n",
    "**TODO 2**: Реализовать предсказание значения *y* на основе одного значения *x*, используя наклон и пересечение. Файл [linreg_simple.py](src/linreg_simple.py) -> функция `predict(x, slope, intercept)`.\n",
    "\n",
    "**TODO 3**: Выполнить линейную регрессию на примере прогнозирования уровня смертности от рака кожи на основе географической широты американских штатов.\n",
    "* Загрузить файл *data/skincancer.csv* и выполнить линейную регрессию в Python файле [todo3.py](src/todo3.py).\n",
    "* Построить график для этого случая.\n",
    "* Попробовать линейную регрессию на том же наборе данных, но вместо географической широты использовать географическую долготу.\n",
    "Примечание: Необходимо установить библиотеки с помощью команды: `pip install -r requirements.txt`\n",
    "\n",
    "**TODO 4**: Решить пример из **TODO 3** с использованием библиотеки [scikit-learn](https://scikit-learn.org) в файле [todo4.py](src/todo4.py).\n",
    "\n",
    "`Scikit-learn` - это *open-source* библиотека на языке Python, представляющая собой простой и эффективный инструмент для машинного обучения и анализа данных. Эта библиотека содержит реализации различных алгоритмов и примеры их применения.\n",
    "\n",
    "\n",
    "#### Множественная линейная регрессия\n",
    "В отличие от простой линейной регрессии, которая использует только одну предикторную переменную *x*, множественная линейная регрессия включает использование двух или более предикторных переменных. На предыдущем примере мы могли бы добавить: как зависит уровень смертности от рака кожи от географической широты **и** цены на солнцезащитный крем, средней зарплаты, климата?\n",
    "\n",
    "Множественная линейная регрессия - отличный инструмент для выявления интересных зависимостей между данными, которые могут быть не очевидны с первого взгляда.\n",
    "\n",
    "В общем виде множественная линейная регрессия включает уравнение:\n",
    "$ \\hat{y} = a + b_1x_1 + b_2x_2 + ... + b_px_p $, где необходимо найти параметры *a* и *b* так же, как и в случае простой линейной регрессии - минимизацией суммы квадратов ошибок предсказания $ SSE=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2 $.\n",
    "\n",
    "Больше о множественной линейной регрессии на следующих занятиях.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
