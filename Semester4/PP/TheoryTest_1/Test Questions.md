## Лекция 1

1) Что такое асимптотические обозначения?
   Асимптотические обозначения используются для описания времени выполнения алгоритма $T(n)$.

2) Перечислите асимптотические обозначения:
	$\Theta$, $O$, $o$, $\Omega$, $\omega$.

4) Объясните обозначение $\Theta$.
   - Обозначение $\Theta$ используется для определения времени выполнения алгоритма в худшем случае. 
   - Оно основано на отбрасывании членов более низкого порядка и игнорировании коэффициента перед ведущим членом.

5) Объясните обозначение $O$.
   - Великое $O$ обозначение используется для определения асимптотической верхней границы заданной функции.
   - Малое $o$ представляет границу, которую предоставляет обозначение $O$. Она может быть асимптотически узкой или нет.

6) Объясните обозначение $\Omega$.
   - Обозначение $\Omega$ используется для определения асимптотической нижней границы заданной функции.
   - $\Omega(g(n))$ предоставляет время выполнения алгоритма в лучшем случае. 
   - Малое $\omega$ обозначает нижнюю границу, которая не является асимптотически узкой.

7) Объясните теорему об асимптотических обозначениях.
   Для любых двух функций $f(n)$ и $g(n)$ справедливо: $f(n) = \Theta(g(n))$ тогда и только тогда, когда $f(n) = O(g(n))$ и $f(n) = \Omega(g(n))$

7) Метод Мастера.
   Метод Мастера решает рекуррентные уравнения вида: $T(n) = aT(n/b) + f(n)$
   - Это уравнение описывает время выполнения алгоритма, который делит проблему размера $n$ на $a$ подпроблем, каждая размера $n/b$ решается рекурсивно
   - Время решения подпроблемы $T(n/b)$
   - $f(n)$ охватывает стоимость разделения проблемы на подпроблемы.

8) Три случая теоремы Мастера.
   Если $f(n) = O(n^{\log_b a-\epsilon})$ => $T(n) = \Theta(n^{\log_b a})$
   Если $f(n) = \Theta(n^{\log_b a})$ => $T(n) = \Theta(n^{\log_b a} \log n)$
   Если $f(n) = \Omega(n^{\log_b a+\epsilon})$ => $T(n) = \Theta(f(n))$
   
   $a = 1$
   $b = 3/2$,
   $f(n) = 1$
   $T(n) = \Theta(n^{\log_{3/2} 1 \cdot \log{n}})$
   $T(n) = \Theta(1 \cdot \log n)$
   $T(n) = \Theta(\log n)$
   
   $n^{\log(3/2)}$
   
   Если $f(n)$ полиномиально меньше, чем $n^{\log(b)a}$, тогда это 1.
   Если $f(n)$ полиномиально равна $n^{\log(b)a}$, тогда это 2.
   А если больше, то это 3. При этом нужно проверить, выполняется ли условие регулярности $Af(n/b) \leq cf(n)$ для $c<1$

## Лекция 2

1) Что такое сортировка вставками?
   - Сортировка вставками относится к классу инкрементальных алгоритмов.
   - Она эффективна для сортировки небольшого количества элементов. 
   - Представляет собой имитацию способа, которым человек сортирует карты в левой руке, где карта в правой сравнивается с картами в левой.

2) Что такое инвариант цикла и его свойства?
   Инвариант цикла используется как доказательство корректности алгоритма. У него есть 3 свойства:
   1. Инициализация - Истинно перед первой итерацией.
   2. Сохранение - Если истинно перед итерацией, остается истинным и после итерации.
   3. Завершение - Когда цикл завершается, инвариант доказывает корректность алгоритма.

3) Инвариант цикла для сортировки вставками.
   1. Инициализация - Перед первой итерацией $j=2$, подмассив состоит из $A[1]$, т.е. один элемент, и он отсортирован.
   2. Сохранение - Увеличение индекса $j$ для следующей итерации не влияет на ранее отсортированный массив.
   3. Завершение - Когда $j > A.length$, выходит из цикла for.

4) Объясните времена при анализе сортировки вставками.
   - Время выполнения алгоритма зависит от размера ввода. Время выполнения = количество операций. 
   - Чем лучше отсортирован входной массив, тем быстрее выполнится алгоритм. Худший случай - когда массив отсортирован в обратном порядке.
   
   Обычно нас интересует только худшее время по 3 причинам:
   1. Какую бы последовательность мы ни выбрали, время выполнения не может быть больше.
   2. Худший случай происходит довольно часто.
   3. Случайное распределение практически так же плохо, как и худший случай.

5) Объясните подход "Разделяй и властвуй".
   Подход "Разделяй и властвуй" использует рекурсивные алгоритмы. На каждом уровне рекурсии есть 3 шага:
   1. Разделение - разделяет проблему на $n$ подпроблем.
   2. Властвование - решает подпроблемы рекурсивно. Если они достаточно малы, решает их напрямую.
   3. Комбинирование - Решение подпроблем объединяется в общее решение исходной проблемы.

6) Объяснение подхода "Разделяй и властвуй" для сортировки вставками.
   1. Разделение - Разделяет массив из $n$ элементов на 2 подмассива из $n/2$ элементов.
   2. Властвование - Сортирует два подмассива рекурсивно.
   3. Комбинирование - Объединяет два отсортированных подмассива в один отсортированный массив.
   Рекурсия спускается до основания, до массива длиной 1, а массив из 1 элемента уже отсортирован.

7) Что такое процедура Merge?
   Требуется $T(n)$ времени, где $n = r - p + 1$ - количество элементов, которые нужно объединить.
	1. Основной шаг - Выбор меньшей карты из верха двух стопок.
	2. Удаление карты из верха и помещение вниз.
	
   Основной шаг повторяется, пока одна стопка не опустеет, затем остаток другой стопки помещается вниз.
   Шаг занимает $\Theta(1)$, потому что все время сравниваются только 2 карты => Всего $\Theta(n)$.

8) Общее рекурсивное уравнение.
*Тут два раза одни и те же формулы: 1 - то как написано в pdf, 2 - то как я предполагаю это должно было выглядеть, но не уверен*
   $Teta(1)$, $n \leq c$
   $T(n) = \begin{cases} aT(n/b) + D(n) + C(n), & \text{иначе} \end{cases}$
$$
\begin{cases}
Teta(1), & n \leq c \\
T(n) = aT(n/b) + D(n) + C(n), & \text{иначе}
\end{cases}
$$

9) Объяснение подхода "Разделяй и властвуй" для сортировки слиянием.
   1. Разделение - Вычисляет середину подмассива $D(n) = Teta(1)$.
   2. Властвование - Два подпроблема размером $n/2$.
   3. Комбинирование - Слияние массива из $n$ элементов требует $Teta(n)$ времени, поэтому $C(n) = Teta(n)$.

   Рекуррентное для сортировки слиянием:
   $Teta(1)$, $n = 1$,
   $T(n) = \begin{cases} 2T(n/2) + Teta(n), & \text{n > 1} \end{cases}$

$$
\begin{cases}
Teta(1), & n = 1 \\
T(n) = 2T(n/2) + Teta(n), & \text{n > 1}
\end{cases}
$$   
   Метод Мастера: $T(n) = Teta(n \log n)$
## Лекция 3
1) Объяснение платформы для динамической параллелизации.
   - Платформа для динамической параллелизации позволяет указывать параллелизм в приложениях.
   - Конкурентная платформа содержит планировщик и поддерживает две абстракции:
	   1. Вложенная параллельность
	   2. Параллельные циклы

2) 4 преимущества модели динамической параллелизации.
	1. Псевдокод с 3 ключевыми словами: parallel, spawn и sync.
	2. Модель обеспечивает квантификацию параллелизма на основе понятий RAD (размер параллелизма) и RASPON (область параллелизма).
	3. Многие параллельные алгоритмы происходят из практики "Разделяй и властвуй".
	4. Многие платформы поддерживают различные варианты динамической параллелизации, например: Cilk, Cilk++, TBB и другие.

3) Параллелизуйте следующий код:
	Fib(n)
	1. if n <= 1
	2.    return n
	3. else
	4.    x = Fib(n - 1)
	5.    y = Fib(n - 2)
	6.    return x + y

	Решение:
	Fib(n)
	1. if n <= 1
	2.    return n
	3. else
	4.    x = spawn Fib(n - 1)
	5.    y = Fib(n - 2)
	6.    sync
	7.    return x + y
	## Лекция 3 (продолжение)

4) Объяснение модели параллельного выполнения.
   - У нас есть граф вычислений G = (V, E)
   - Вершины V - это инструкции
   - Рёбра E - зависимость между инструкциями
   - Если в G существует направленный путь от узла 'u' до узла 'v', две строки находятся в логической последовательности. В противном случае они параллельны.

5) Какие 4 типа рёбер графа вычислений.
   - Ребро продолжения(u, u') - рисуется вправо, соединяет строку 'u' с 'u'
   - Ребро мержения(u, v) - рисуется вниз и показывает, что строка выполнения 'u' разветвила строку 'v'
   - Ребро вызова - рисуется вниз и представляет обычный вызов процедуры
   - Ребро возврата(u, x) - рисуется вверх и показывает, что строка выполнения 'u' возвращается к своей вызывающей процедуре 'x'

6) Какие меры производительности параллельного алгоритма?
   Меры производительности параллельного алгоритма:
   1. Рад - общее время вычислений на одном процессоре. Если время для каждой вершины 1, то Rad = количество вершин.
   2. Распон - максимальное время, необходимое для выполнения строк по любому пути. Если время для каждой вершины 1, то Распон = количество вершин на самом длинном пути.

7) Что такое критический путь?
   Критический путь графа - это самый длинный путь в графе.

8) Какие времена существуют и кратко опишите их.
   Существует Tp, T1, T(inf):
   1. Tp - Время на P процессоров
   2. T1 - Время на 1 процессоре
   3. T(inf) - Время на неограниченном числе процессоров, в теории каждая строка будет иметь свой процессор.
   
   Рад и распон обеспечивают нижние границы для времени выполнения Tp - закон рада и распона.

9) Формулировка закона Рада.
   Закон РАДА: $Tp \geq \frac{T1}{P}$

10) Формулировка закона Распона.
    Закон РАСПОНА: $Tp \geq T(\infty)$

11) Объяснение закона ускорения.
    Ускорение на $P$ процессорах - это отношение $\frac{T1}{Tp}$ и указывает, во сколько раз выполнение быстрее на $P$ процессорах.
    Если $\frac{T1}{Tp} = \Theta(P)$ => Линейное ускорение
    Если $\frac{T1}{Tp} = P$ => Идеальное линейное ускорение

12) Что такое параллелизм и какие перспективы существуют.
    Параллелизм - это отношение $\frac{T1}{T(\infty)}$
    Три перспективы:
    1. Как отношение - среднее количество работы
    2. Как верхняя граница - максимальное ускорение
    3. Ограничение идеального линейного ускорения - максимальное количество процессоров для идеального линейного ускорения

13) Что такое свободность параллелизма.
    Свободность параллелизма представляет отношение $\left(\frac{T1}{T(\infty)}\right)/P$.
    Фактор, с которым параллелизм алгоритма превосходит количество процессоров.
    Если свободность меньше 1, идеальное линейное ускорение невозможно. Чем ближе к 1, тем ближе к идеальному ускорению.

14) Что такое жадное планирование и какие шаги включает.
    Жадное планирование - назначает как можно больше строк на каждом шаге.
    Шаги:
    1. Полный
    2. Неполный

15) Объяснение и формулировка Теоремы о верхней границе Tp.
    Теорема о верхней границе Tp: На $P$ процессорах, жадное планирование выполняет параллельный алгоритм с радом $T1$ и распоном $T(\infty)$ за время:
    $Tp \leq \frac{T1}{P} + T(\infty)$
    
16) Объясните гонку данных
	Гонка данных - происходит между двумя логически параллельными инструкциями, которые обращаются к одной и той же области памяти, и по крайней мере одна из этих инструкций выполняется с записью в эту область памяти.
## Лекция 4

1) Рад, Распон и Параллелизм прямого алгоритма.
   - Рад: Код с тремя вложенными циклами с по $n$ итераций => $T1(n) = \Theta(n^3)$
   - Распон: $T(\infty)(n) = \Theta(n)$, так как распределение для parallel_for $\Theta(\log n)$, а для обычного $\Theta(n)$; берется максимальное время.
   - Параллелизм: $\frac{\Theta(n^3)}{\Theta(n)} = \Theta(n^2)$

2) Метод Штрассена для умножения матриц - объяснение.
   Ключ в том, чтобы сделать рекурсивное дерево менее разветвленным. Вместо 8 умножений матриц $n/2$ на $n/2$ он выполняет 7. Цена за удаление одного умножения матрицы: несколько матричных сложений, но это количество постоянно.

3) Шаги метода Штрассена.
   Метод состоит из 4 шагов:
   1. Разделить матрицы $A$, $B$, $C$ на подматрицы $n/2 \times n/2$. Этот шаг занимает время $\Theta(1)$.
   2. Создать 10 матриц - $\Theta(n^2)$ времени.
   3. Рекурсивно вычислить 7 матриц.
   4. Вычислить желаемые подматрицы для матрицы $C$ - $\Theta(n^2)$ времени.

4) Рад, Распон и Параллелизм P-Matrix-Multiply-Rec.
   - Рад: Разделение матрицы занимает время $\Theta(1)$. Восемь рекурсивных умножений подматриц и их сложение. $T1(n) = 8 T1(n/2) + \Theta(n^2)$. Согласно мастер-теореме: $T1(n) = \Theta(n^3)$.
   - Распон: $T(\infty)(n) = \Theta(\log^2 n)$ методом замены.
   - Параллелизм: $\frac{T1(n)}{T(\infty)(n)} = \Theta\left(\frac{n^3}{\log^2 n}\right)$

5) Рад, Распон и Параллелизм метода Штрассена.
   - Рад: $T1(n) = \Theta(n^{\log 7})$.
   - Распон: Семь рекурсивных вызовов выполняются параллельно. Получается такая же рекуррентность, как и для P-M-M-R: $T(\infty)(n) = \Theta(\log^2 n)$.
   - Параллелизм: $\frac{T1(n)}{T(\infty)(n)} = \Theta\left(\frac{n^{\log 7}}{\log^2 n}\right)$

6) Рад, Распон и Параллелизм сортировки слиянием (Merge Sort).
   - Рад: $T1(n) = 2T1(n/2) + \Theta(n) = \Theta(n \log n)$.
   - Распон: Поскольку два рекурсивных вызова могут выполняться параллельно, $T(\infty)(n) = T(\infty)(n/2) + \Theta(n) = \Theta(n)$.
   - Параллелизм: $\frac{T1(n)}{T(\infty)(n)} = \Theta(\log n)$

7) Объяснение Бинарного поиска.
   Вызов процедуры занимает $\Theta(\log n)$ последовательного времени в худшем случае. $n = r - p + 1$ - размер подмассива, на котором выполняется процедура. Поскольку Бинарный поиск - это последовательная процедура, её рад и распон в худшем случае оба равны $\Theta(\log n)$.
## Лекция 5

1) Шаги в параллелизации программы:
   1. Декомпозиция - разбиение задачи на несколько взаимно независимых подзадач.
   2. Назначение - группировка задач в процессы.
   3. Оркестрация - решение передачи данных между процессами, которые производят данные, и теми, которые используют эти данные.
   4. Отображение - процессов на ядра многопроцессорной системы.
2) ?
	1. Декомпозиция имеет целью разбить задачу на несколько взаимно независимых подзадач.
	2. Распределение имеет целью группировку задач в процессы.
	3. Оркестрация решает проблему передачи данных между процессами, которые производят данные, и теми, которые эти данные используют.
	4. Сопоставление процессов с ядрами многопроцессорной системы.

3) ?
- Условие Бернштейна
- Выражение алгоритма
- Поиск параллелизма
- Структура алгоритма
- Построение программы
- Вспомогательные структуры
- Исполнительные механизмы

4) Типы декомпозиции:
   Типы декомпозиции, которые могут использоваться для обнаружения параллелизма:
   1. Декомпозиция задач
   2. Декомпозиция данных
   3. Декомпозиция потоковой обработки

5) ?
	Применяем шаблоны декомпозиции.
		Декомпозиция задач, соответствующих одному обновлению атома (одна итерация цикла).
	Шаблоны анализа зависимостей.
		Анализ зависимостей управления
		Анализ зависимостей данных
	Оценка проекта
		Учитываем целевую архитектуру
		Анализируем, обладают ли данные пространственными свойствами для эффективной обработки их зависимостей

6) Условие Бернштейна.
	У нас есть один набор ячеек памяти Ri, из которых задача 1 считывает данные, и набор ячеек памяти Wj, в которые записывает данные другая задача, что эти две задачи могут выполняться параллельно тогда и только тогда, когда выполняются условия:
	R – вход
	W – выход
	а) пересечение R1 и W2 — пустое множество
	б) R2 пересечение W1 — пустое множество
	в) W1 пересечение W2 — пустое множеств

7) Названия шаблонов проектирования
	Параллелизм задач, разделяй и властвуй, геометрическая декомпозиция, рекурсивные данные, потоковая обработка, события на основе координации.

8) поддерживающие структуры
	SPMD (одна программа и множество данных);
	Параллелизм цикла;
	Master/Worker
	Fork/Join

9) Типы редукции
	Последовательное сокращение
	Древовидное сокращение
	Рекурсивное удвоение сокращения