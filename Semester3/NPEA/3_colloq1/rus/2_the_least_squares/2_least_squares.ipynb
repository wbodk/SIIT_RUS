{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод наименьших квадратов\n",
    "*Јована Арсеновић, Николина Живановић, Анастасија Голић,*\n",
    "*Вељко Радојичић, Стефан Топалов*\n",
    "*20. ноябрь 2023.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Элементы теории ошибок\n",
    "\n",
    "Все больше реальных проблем во всех областях жизни сегодня решается математическим моделированием и их симуляцией, благодаря интенсивному развитию компьютеров. Если бы мы хотели моделировать процесс, измерения которого представлены на рисунке 1, самым простым способом было бы использование процедуры линейной регрессии, и тогда связь между характеристиками можно было бы представить в линейной форме:\n",
    "$$ y = a_1x + a_0, \\tag{1}$$\n",
    "где $a_1$ и $a_0$ - неизвестные параметры, которые мы хотим определить в процедуре оптимизации, при этом параметр $a_1$ представляет угол наклона кривой, а параметр $a_0$ - точку пересечения с ординатой.\n",
    "\n",
    "<center>\n",
    "\n",
    "![**Рисунок 1:** Иллюстрация линейной регрессии - подгонка данных полиномом первого порядка.](leastSquaresMethod.jpeg)\n",
    "\n",
    "\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "\n",
    "**Фигура 1: Иллюстрация линейной регрессии - аппроксимация данных полиномом первого порядка.**\n",
    "\n",
    "</center>\n",
    "\n",
    "В течение процесса оптимизации необходимо найти те значения параметров $a_1$ и $a_0$, которые позволят прямой наилучшим образом описать данный процесс, то есть чтобы разница между реальным и аппроксимированным значением была минимальной:\n",
    "$$ E_i = |y - y_i|. \\tag{2}$$\n",
    "\n",
    "Этот тип ошибки называется абсолютной ошибкой и может быть представлен следующим образом:\n",
    "\n",
    "1. **Максимальная абсолютная ошибка:**\n",
    "$$ E_\\infty(f) = \\max \\left| y(x_k) - y_k \\right|, \\quad 1 < k < n $$\n",
    "\n",
    "2. **Средняя абсолютная ошибка:**\n",
    "$$ E_1(f) = \\frac{1}{n} \\sum_{k=1}^{n} \\left| y(x_k) - y_k \\right|^2 $$\n",
    "\n",
    "3. **Среднеквадратичная абсолютная ошибка:**\n",
    "$$ E_2(f) = \\sqrt{\\frac{1}{n} \\sum_{k=1}^{n} |f(x_k) - y_k|^2} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод наименьших квадратов\n",
    "\n",
    "Начнем с процедуры линейной регрессии, опираясь на метод наименьших квадратов (поскольку эти процедуры являются основой машинного обучения) и представим их как задачу оптимизации. С самого начала необходимо определить критерий оптимальности и представить его в квадратичной форме, где цель - минимизировать ошибку:\n",
    "\n",
    "$$ F = \\sum_{k=1}^{n} \\left( y(x_k) - y_k \\right)^2 \\tag{3}$$ \n",
    "\n",
    "Поскольку мы решаем проблему линейной регрессии, критерий оптимальности теперь принимает форму:\n",
    "\n",
    "$$ F = \\sum_{k=1}^{n} \\left( a_1x_k + a_0 - y_k \\right)^2 \\tag{4}$$\n",
    "\n",
    "Как мы ранее отметили, наша цель - определить оптимальные значения параметров $a_0$ и $a_1$, чтобы критерий оптимальности (2) был минимален. На основе необходимых условий экстремума получаем следующую систему уравнений:\n",
    "\n",
    "$$ \\begin{cases}\n",
    "\\frac{\\partial F}{\\partial a_0} = \\sum_{k=1}^{n} 2(a_1x_k + a_0 - y_k) = 0 \\\\\n",
    "\\frac{\\partial F}{\\partial a_1} = \\sum_{k=1}^{n} 2(a_1x_k + a_0 - y_k)x_k = 0\n",
    "\\end{cases} \\tag{5}$$\n",
    "\n",
    "Линейная система уравнений (5) также может быть записана в матричной форме:\n",
    "\n",
    "$$ \\begin{bmatrix} n & \\sum_{k=1}^{n} x_k \\\\ \\sum_{k=1}^{n} x_k & \\sum_{k=1}^{n} x_k^2 \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\end{bmatrix} = \\begin{bmatrix} \\sum_{k=1}^{n} y_k \\\\ \\sum_{k=1}^{n} x_ky_k \\end{bmatrix} \\tag{6} $$\n",
    "\n",
    "Решим ее методом Краммера: \n",
    "\n",
    "Mетод Краммера представляет собой теорему в линейной алгебре, которая предоставляет решения системы линейных уравнений с использованием детерминант. Если систему уравнений представить в виде умножения матриц: $Ax = c$, где $A$ - квадратная матрица, $x$ - вектор-столбец переменных, и матрица $A$ является регулярной (невырожденной), то решение можно выразить следующим образом:\n",
    "\n",
    "$$x_i = \\frac{\\text{det}(A_i)}{\\text{det}(A)}$$\n",
    "\n",
    "где $A_i$ - матрица, полученная заменой $i$-го столбца в матрице $A$ вектором-столбцом $c$.\n",
    "\n",
    "В дальнейшем мы рассмотрим решение матричного уравнения (8) и его дальнейшее применение в изучении проблемы регрессии. Решение системы (6) получается с использованием правила Крамера. После решения системы уравнений и получения оптимальных значений параметров $a_0$ и $a_1$ формально завершается процесс линейной регрессии.\n",
    "\n",
    "$$ a_1 = \\frac{n \\sum_{k=1}^{n} x_k y_k - \\sum_{k=1}^{n} x_k \\sum_{k=1}^{n} y_k}{n \\sum_{k=1}^{n} x_k^2 - (\\sum_{k=1}^{n} x_k)^2} \\tag{7a}$$\n",
    "\n",
    "$$ a_0 = \\frac{\\sum_{k=1}^{n} y_k - a_1 \\sum_{k=1}^{n} x_k}{n} \\tag{7b}$$\n",
    "\n",
    "Где $a_0^*$ и $a_1^*$ обозначают оптимальные значения параметров $a_0$ и $a_1$ в соответствии с предполагаемым критерием оптимальности (2). Оптимальное значение критерия обозначим как $F_{LSE}$ и вычислим его следующим образом:\n",
    "\n",
    "$$ F_{LSE} = \\sum_{i=1}^{n} (a_1^*x_i + a_0^* - y_i)^2 \\tag{8} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Линеаризация нелинейных моделей.\n",
    "\n",
    "Логично предположить, что связь между входом $ x_i $ и выходом $ y_i $ не всегда линейна. Однако существует широкий класс нелинейных проблем, где с подходящими изменениями начальная проблема нелинейной регрессии может быть приведена к линейной.\n",
    "\n",
    "Если мы хотим аппроксимировать начальную проблему полиномом второй степени:\n",
    "\n",
    "$$ y = a_2x^2 + a_1x + a_0 \\tag{9} $$\n",
    "\n",
    "Тогда критерий оптимальности будет иметь следующую форму:\n",
    "\n",
    "$$ F = \\sum_{k=1}^{n} (a_2x_k^2 + a_1x_k + a_0 - y_k)^2 \\tag{10} $$\n",
    "\n",
    "Простой математикой мы получаем условия экстремума, приравнивая частные производные к нулю. Оптимальные значения параметров $ a_2, a_1 $ и $ a_0 $ также получаются с использованием правила Крамера. Оставим читателя с упражнением:\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial F}{\\partial a_0} = \\sum_{k=1}^{n} 2(a_2x_k^2 + a_1x_k + a_0 - y_k) = 0 \\tag{11a} $$\n",
    "$$ \\frac{\\partial F}{\\partial a_1} = \\sum_{k=1}^{n} 2(a_2x_k^2 + a_1x_k + a_0 - y_k)x_k = 0 \\tag{11b} $$ \n",
    "$$ \\frac{\\partial F}{\\partial a_2} = \\sum_{k=1}^{n} 2(a_1x_k + a_0 - y_k)x_k^2 = 0 \\tag{11с} $$\n",
    "\n",
    "Экспоненциальную модель можно представить следующим образом:\n",
    "\n",
    "$$ y(x) = Ce^{Ax} $$\n",
    "\n",
    "Если мы вставим эту модель в критерий оптимальности (3):\n",
    "\n",
    "$$ F = \\sum_{k=1}^{n} (Ce^{Ax_k} - y_k)^2 \\tag{12} $$\n",
    "\n",
    "Необходимые условия экстремума получаются дифференцированием критерия по всем переменным:\n",
    "\n",
    "$$ \\frac{\\partial F}{\\partial A} = \\sum_{k=1}^{n} 2(Ce^{Ax_k} - y_k)Cx_ke^{Ax_k} = 0 \\tag{13a} $$\n",
    "\n",
    "$$ \\frac{\\partial F}{\\partial C} = \\sum_{k=1}^{n} 2(Ce^{Ax_k} - y_k)e^{Ax_k} = 0 \\tag{13b} $$\n",
    "\n",
    "Очевидно, что это нельзя записать в матричной форме, поэтому мы прибегаем к процессу аппроксимации - линеаризации.\n",
    "\n",
    "Линеаризация этой модели естественна и довольно проста. Если мы возьмем натуральный логарифм предыдущего выражения, мы получим:\n",
    "\n",
    "$$ \\ln y = \\ln C + Ax $$\n",
    "\n",
    "Теперь очевидно, что новое $ Y = \\ln y $, а коэффициенты из уравнений (7) - это $ a_0 = \\ln C $ и $ a_1 = A $, то есть:\n",
    "\n",
    "$$ Y = a_0 + a_1 x $$\n",
    "\n",
    "Применяя уравнения (7), мы легко определяем оптимальные значения параметров линеаризованной модели, то есть вычисляем $ a_0 $ и $ a_1 $, что в конечном итоге дает экспоненциальную регрессионную модель:\n",
    "\n",
    "$$ y(x) = Ce^{Ax} = e^{a_0}e^{a_1x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример\n",
    "\n",
    "Во время экзамена были собраны данные о студентах, сколько времени они потратили на подготовку к экзамену и сколько баллов они получили, и эти данные представлены в таблице.\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|}\n",
    "    \\hline\n",
    "    \\text{SATI} & \\text{BODOVI} \\\\\n",
    "    \\hline\n",
    "    6 & 82 \\\\\n",
    "    10 & 88 \\\\\n",
    "    2 & 56 \\\\\n",
    "    4 & 64 \\\\\n",
    "    0 & 23 \\\\\n",
    "    \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**(a) Линейная аппроксимация:**\n",
    "\n",
    "Поскольку требуется представить данные линейно, необходимо определить параметры прямой. Для линейной аппроксимации данных необходимо найти оптимальные значения $a_0$ и $a_1$, используя выражения (7):\n",
    "\n",
    "$$\n",
    "a_1 = \\frac{5(6 \\cdot 82 + 10 \\cdot 88 + 2 \\cdot 56 + 4 \\cdot 64) - (6 + 10 + 2 + 4)(82 + 88 + 56 + 64 + 23)}{5(36 + 100 + 4 + 16) - (6 + 10 + 2 + 4)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_0 = \\frac{82 + 88 + 56 + 64 + 23 - a_1(6 + 10 + 4 + 2)}{5}\n",
    "$$\n",
    "\n",
    "Решив эти уравнения, получаем $a_1 \\approx 6.1284$ и $a_0 \\approx 35.6351$. Таким образом, уравнение прямой имеет вид $y = 6.1284x + 35.6351$. На основе этого можно сделать вывод, что для сдачи экзамена необходимо потратить 2.5 часа на подготовку.\n",
    "\n",
    "**(b) Экспоненциальная аппроксимация:**\n",
    "\n",
    "Сначала необходимо определить $Y$ как $\\ln(y)$:\n",
    "\n",
    "$$ Y = \\ln(y) = [4.4067, 4.4773, 4.0254, 4.1589, 3.1355] $$\n",
    "\n",
    "Это подставляется в выражение (7), и получаем оптимальные значения параметров $a_1^* = 0.1183$ и $a_0^* = 3.5202$. На основе этого можно вычислить $C = e^{a_0^*} = 33.7927$, и получить экспоненциальную кривую $y = 33.7927e^{0.1183x}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
